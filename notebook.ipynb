{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will be using the [Amazon Review Data (2018)](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/) dataset provided by the University of California San Diego. It consists of a total number of 233.1 million Amazon reviews written between May 1996 and Oct 2018 across 29 product categories. The dataset is available in the form of two JSON files for each product category, with each line in a file representing a JSON object. One file contains the reviews and the other contains the metadata for the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "categories=(\"AMAZON_FASHION\" \"All_Beauty\" \"Appliances\" \"Arts_Crafts_and_Sewing\" \"Automotive\" \"Books\" \"CDs_and_Vinyl\" \"Cell_Phones_and_Accessories\" \"Clothing_Shoes_and_Jewelry\" \"Digital_Music\" \"Electronics\" \"Gift_Cards\" \"Grocery_and_Gourmet_Food\" \"Home_and_Kitchen\" \"Industrial_and_Scientific\" \"Kindle_Store\" \"Luxury_Beauty\" \"Magazine_Subscriptions\" \"Movies_and_TV\" \"Musical_Instruments\" \"Office_Products\" \"Patio_Lawn_and_Garden\" \"Pet_Supplies\" \"Prime_Pantry\" \"Software\" \"Sports_and_Outdoors\" \"Tools_and_Home_Improvement\" \"Toys_and_Games\" \"Video_Games\")\n",
    "data_links=()\n",
    "meta_links=()\n",
    "\n",
    "for category in \"${categories[@]}\"\n",
    "do\n",
    "    data_links+=(\"https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/$category.json.gz\")\n",
    "    meta_links+=(\"https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/metaFiles2/meta_$category.json.gz\")\n",
    "done\n",
    "\n",
    "mkdir -p /data/json/\n",
    "wget -P /data/json/ \"${data_links[@]}\" \"${meta_links[@]}\"\n",
    "\n",
    "gunzip /data/json/*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above script, we download the data and the metadata files for each category, store them in the `/data/json/` directory and unzip them. The resulting JSON files amount to a total of almost 300GB of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded these huge files, we cannot simply load them into memory and start working with them. One way of dealing with this problem is to use [Spark](https://spark.apache.org/) on top of the [Hadoop Distributed File System (HDFS)](https://hadoop.apache.org/), which allows us to distribute the data and the computations with the data. Therefore, the following steps require a Spark and Hadoop installation on the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sed -i -e 's/:\":/\":/g' /data/json/*.json\n",
    "\n",
    "sed -i -e 's/\"size\":/\"_size\":/g' /data/json/meta_AMAZON_FASHION.json\n",
    "sed -i -e 's/\"size\":/\"_size\":/g' /data/json/meta_Clothing_Shoes_and_Jewelry.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to work with the data using Spark, we have to rename some fields in the JSON files, because their names are not compatible with Spark DataFrames.\n",
    "\n",
    "Some of the fields contain a `:` at the end of their name, which was likely introduced by some mistake in the data collection process and is not allowed in Spark. We remove the `:` from the field names with the first command.\n",
    "\n",
    "There are also fields called `size` in the metadata files of the categories `AMAZON_FASHION` and `Clothing_Shoes_and_Jewelry`, which is a reserved keyword in Spark. Therfore, we rename these fields to `_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, ArrayType, LongType\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"8g\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"28g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start working with the data in spark. We create a new session with memory configurations according to our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['AMAZON_FASHION', 'All_Beauty', 'Appliances', 'Arts_Crafts_and_Sewing', 'Automotive', 'Books', 'CDs_and_Vinyl', 'Cell_Phones_and_Accessories', 'Clothing_Shoes_and_Jewelry', 'Digital_Music', 'Electronics', 'Gift_Cards', 'Grocery_and_Gourmet_Food', 'Home_and_Kitchen', 'Industrial_and_Scientific', 'Kindle_Store', 'Luxury_Beauty', 'Magazine_Subscriptions', 'Movies_and_TV', 'Musical_Instruments', 'Office_Products', 'Patio_Lawn_and_Garden', 'Pet_Supplies', 'Prime_Pantry', 'Software', 'Sports_and_Outdoors', 'Tools_and_Home_Improvement', 'Toys_and_Games', 'Video_Games']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original data is partitioned into two files for each category, we need an array of the names of all categories called `categories` in order to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('asin', StringType()),\n",
    "    StructField('image', ArrayType(StringType())),\n",
    "    StructField('overall', DoubleType()),\n",
    "    StructField('reviewText', StringType()),\n",
    "    StructField('reviewTime', StringType()),\n",
    "    StructField('reviewerID', StringType()),\n",
    "    StructField('reviewerName', StringType()),\n",
    "    StructField('style', StructType([\n",
    "        StructField('Color', StringType()),\n",
    "        StructField('Color Name', StringType()),\n",
    "        StructField('Design', StringType()),\n",
    "        StructField('Flavor', StringType()),\n",
    "        StructField('Format', StringType()),\n",
    "        StructField('Item Package Quantity', StringType()),\n",
    "        StructField('Package Quantity', StringType()),\n",
    "        StructField('Package Type', StringType()),\n",
    "        StructField('Pattern', StringType()),\n",
    "        StructField('Scent Name', StringType()),\n",
    "        StructField('Size', StringType()),\n",
    "        StructField('Size Name', StringType()),\n",
    "        StructField('Style', StringType()),\n",
    "        StructField('Style Name', StringType()),\n",
    "    ])),\n",
    "    StructField('summary', StringType()),\n",
    "    StructField('unixReviewTime', LongType()),\n",
    "    StructField('verified', BooleanType()),\n",
    "    StructField('vote', StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to combine all the available data into a single (yet distributed) file. However, the structure of the data files is slightly different for each category, which leads to problems when we try to combine them. Therefore, we first have to unify the structure of the data files, which we define as the Spark struct `schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    print(f'Processing {category}')\n",
    "\n",
    "    data = ps.read_json(f'/data/json/{category}.json', schema=schema, index_col=['reviewerID', 'asin'])\n",
    "\n",
    "    meta = ps.read_json(f'/data/json/meta_{category}.json', index_col='asin')\n",
    "    meta = meta.drop(['similar_item', 'details', 'tech1', 'tech2'], axis=1)\n",
    "\n",
    "    df = data.reset_index().join(meta, on='asin')\n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index(['reviewerID', 'asin'], inplace=True)\n",
    "    df['category'] = category\n",
    "    df.to_parquet(f'/data/{category}.parquet', index_col=['reviewerID', 'asin'])\n",
    "\n",
    "    print(f'Finished {category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simply read the data and metadata files for each category into a Pandas on Spark DataFrame, join them and write the resulting DataFrame to a parquet file.\n",
    "\n",
    "Pandas on Spark is an abstraction of Spark DataFrames, which can be used the same way the popular Python library `pandas` is used, with the added benefit of being able to distribute the data and the computations with the data.\n",
    "\n",
    "The parquet file is a columnar storage format, which is optimized for distributed data processing. It is also the default file format for Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data in a clean structure and stored in an efficient file format, we can work with it well. However, since we still have a separate file for each category, we don't want to have to work with all these files separately. Therefore, we concatenate all the parquet files into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [ps.read_parquet(f'/data/{category}.parquet', index_col=['reviewerID', 'asin']) for category in categories]\n",
    "df = ps.concat(df_list)\n",
    "df.to_parquet('/data/data.parquet', index_col=['reviewerID', 'asin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join the files, we can simply read the separate parquet files into DataFrames, concatenate them and write the resulting DataFrame to a new parquet file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
